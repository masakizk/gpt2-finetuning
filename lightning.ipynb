{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import lightning.pytorch as pl\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "from itertools import chain\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL=\"rinna/japanese-gpt2-medium\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL, use_fast=False)\n",
    "tokenizer.do_lower_case = True  # due to some bug of tokenizer config loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_texts(texts: List[str]):\n",
    "    # 全ての文書を結合し、トークン化\n",
    "    examples = tokenizer.batch_encode_plus(texts)\n",
    "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "\n",
    "    # トークン化された文書をブロックサイズに分割\n",
    "    block_size = tokenizer.model_max_length\n",
    "    if block_size > 1024:\n",
    "        block_size = 1024\n",
    "    \n",
    "    total_length = len(concatenated_examples[\"input_ids\"])\n",
    "    if total_length >= block_size:\n",
    "        total_length = (total_length // block_size) * block_size\n",
    "\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.DataLoader):\n",
    "    def __init__(self, filepath):\n",
    "        with open(filepath) as f:\n",
    "            self.data = group_texts(f.readlines())\n",
    "    \n",
    "    def __len__(self):\n",
    "        keys = list(self.data.keys())\n",
    "        return len(self.data[keys[0]])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {k: torch.tensor(v[idx]) for k, v in self.data.items()}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModule(pl.LightningDataModule):\n",
    "    def __init__(self, train, val, test, batch_size=4, num_workers=4):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.device = device\n",
    "\n",
    "        self.train = train\n",
    "        self.val = val\n",
    "        self.test = test\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        if stage == 'fit' or stage is None:\n",
    "            self.train_dataset = Dataset(self.train)\n",
    "\n",
    "        if stage == 'validate' or stage is None:\n",
    "            self.val_dataset = Dataset(self.val)\n",
    "        \n",
    "        if stage == 'test' or stage is None:\n",
    "            self.test_dataset = Dataset(self.test)\n",
    "\n",
    "        if stage == 'predict' or stage is None:\n",
    "            self.predict_dataset = Dataset(self.test)\n",
    "\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.train_dataset, \n",
    "            batch_size=self.batch_size, \n",
    "            num_workers=self.num_workers,\n",
    "            collate_fn=self.collate_fn,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.val_dataset, \n",
    "            batch_size=self.batch_size, \n",
    "            num_workers=self.num_workers,\n",
    "            collate_fn=self.collate_fn,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.test_dataset, \n",
    "            batch_size=self.batch_size, \n",
    "            num_workers=self.num_workers,\n",
    "            collate_fn=self.collate_fn,\n",
    "        )\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        keys = batch[0].keys()\n",
    "        data = {k: torch.stack([b[k] for b in batch]) for k in keys}\n",
    "        return data[\"input_ids\"], data[\"labels\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(MODEL).to(self.device)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        return self.model(x, labels=y)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        self.model.train()\n",
    "        x, y = batch\n",
    "        outputs = self(x, y)\n",
    "        loss = outputs.loss\n",
    "        return {\n",
    "            'loss': loss,\n",
    "        }\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self.model.eval()\n",
    "        x, y = batch\n",
    "        outputs = self(x, y)\n",
    "        loss = outputs.loss\n",
    "        return {\n",
    "            'loss': loss,\n",
    "        }\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        self.model.eval()\n",
    "        x, y = batch\n",
    "        outputs = self(x, y)\n",
    "        loss = outputs.loss\n",
    "        return {\n",
    "            'loss': loss,\n",
    "        }\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['loss'] for x in outputs]).mean()\n",
    "        self.log('val_loss', avg_loss)\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['loss'] for x in outputs]).mean()\n",
    "        self.log('test_loss', avg_loss)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-5, betas=(0.9, 0.999), eps=1e-08)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitProgressBar(pl.callbacks.TQDMProgressBar):\n",
    "    def init_validation_tqdm(self):\n",
    "        return tqdm(disable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zacker/anaconda3/envs/gpt_rinna_finetuning/lib/python3.7/site-packages/lightning/pytorch/trainer/connectors/accelerator_connector.py:479: LightningDeprecationWarning: Setting `Trainer(gpus=1)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=1)` instead.\n",
      "  f\"Setting `Trainer(gpus={gpus!r})` is deprecated in v1.7 and will be removed\"\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GPT2().to(device)\n",
    "\n",
    "datamodule = DataModule(\n",
    "    train='data/train.txt',\n",
    "    val='data/train.txt',\n",
    "    test='data/train.txt',\n",
    "    batch_size=1,\n",
    ")\n",
    "datamodule.setup()\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    gpus=1,\n",
    "    accelerator='gpu',\n",
    "    max_epochs=3,\n",
    "    # overfit_batches=1,\n",
    "    logger=pl.loggers.TensorBoardLogger('logs/', name='gpt2'),\n",
    "    callbacks=[LitProgressBar()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type            | Params\n",
      "------------------------------------------\n",
      "0 | model | GPT2LMHeadModel | 336 M \n",
      "------------------------------------------\n",
      "336 M     Trainable params\n",
      "0         Non-trainable params\n",
      "336 M     Total params\n",
      "1,344.512 Total estimated model params size (MB)\n",
      "2023-08-24 15:29:15.537455: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-24 15:29:15.717268: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-08-24 15:29:16.293218: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-08-24 15:29:16.293398: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-08-24 15:29:16.293401: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "680fa810849549f3a1014ab8c2a1dda7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zacker/anaconda3/envs/gpt_rinna_finetuning/lib/python3.7/site-packages/lightning/pytorch/trainer/trainer.py:1613: PossibleUserWarning: The number of training batches (14) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  category=PossibleUserWarning,\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "593902395ae84f008c63fe4e352f2b7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(\n",
    "    model=model,\n",
    "    datamodule=datamodule,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "おはよう、お兄ちゃん。</s> 昨日、お兄ちゃんと妹と3人で夕飯を食べました。 1番の食べ場所は、やっぱり、やっぱりカレー。 3つの食べ方 カレー スープカレー カレー 玉子焼き スパゲッティー カレーパン カレーに合うおつまみ カレーと... #お兄ちゃんのことなんかぜんぜん好きじゃないんだからねっ!! #10回読んでしまった #\n",
      "おはよう、お兄ちゃん。</s> #11 <unk> い月だよね、みんな。 <unk> い月だよね、みんな。 「おはようお兄ちゃん、お兄ちゃん。 #11 ねぇ、お兄ちゃん。 」(おはよう、お兄ちゃん。 )は、マミたんの5thシングル。 2013年5月24日にlantisから発売された。 前作「あなたがいるなら」から約2か月ぶりのリリースとなる2013年2作目のシングル。 表題曲「<unk> い\n",
      "おはよう、お兄ちゃん。</s> ちょっとだけ、お姉ちゃんって呼んでいい? 僕の唇にキスしてくれない? ふちに...僕の精液が...ふちに...</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
      "おはよう、お兄ちゃん。</s> とってもいい天気! 今日も1日雨降らないといいなぁ(^-^; とりあえずお兄ちゃん、仕事する?お昼に会いたいからね～</s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
      "おはよう、お兄ちゃん。</s> 久々の更新、相変わらず忙しい日々ですが、そんな日々も悪くありませんね。 今日も、いつものようにご飯食べて、パソコンして、テレビ観たり。(・∀・) いやいや、そんなわけで、今日は、普段忙しいお兄ちゃんの代わりに、「ごちうさ 」 を見てきます。 そういえば、さっきまで、 「ごちうさ 」 を\n",
      "おはよう、お兄ちゃん。</s> が、 さっきまで 俺のこと 馬鹿にしていたのか。 「 何言ってんだか知らないけど、そういう問題じゃない 」 今、あなたは 「 じゃあ、 そっち 」 と 私に 言いたかったのね。 さっきまで、 「 んなわけねえよ、 」 って 私に話したかったの? そっかぁ、わかってくれて すごく嬉しい\n",
      "おはよう、お兄ちゃん。</s> 1話。昨日は、久しぶりに、本屋さんに行って、本を買った。(いや、本を買うこと自体が、久しぶりだし。)でも、今週は、まだ、このシリーズ1話が読めない。(もう1話しかない)とりあえず、今日は、ちょっとだけ、読んでみた。 あら、そう言われると、最近、あんなに読んでなかったわね、って\n",
      "おはよう、お兄ちゃん。</s> ちゃんと眠れるよ...? ふぅ、眠くなっちゃった! もう、こんな時間かぁ...今日も、お兄ちゃん、...............いーな? あ、今のところは...お兄ちゃん、ちょっと...だけ、今夜は寝ないことにするかな? いーいいよぉ...あ、今のところ...お兄ちゃん、ふぅ、ちょっとだけ...いいよぉ.........\n"
     ]
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "input = tokenizer.encode(\"おはよう、お兄ちゃん。\", return_tensors=\"pt\").to(device)\n",
    "output = model.model.generate(input, max_length=100, do_sample=True, top_p=0.95, top_k=60, num_return_sequences=8)\n",
    "for text in tokenizer.batch_decode(output):\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "おはよう、お兄ちゃん。</s> 」(おにいちゃん。 )は、原作:有元カゲロウ、漫画:北岡まりあによる日本の漫画。 『ちゃお』(小学館)で2007年6月号から2007年8月号まで連載、単行本は全11巻。 有元カゲロウが『ちゃお』でのデビューから10周年を迎えた2007年11月号に「ちゃお 特別編」として『ちゃお』vol. 2に掲載された。 作者のtwitterでは、同誌\n"
     ]
    }
   ],
   "source": [
    "gpt2 = AutoModelForCausalLM.from_pretrained(MODEL).to(device)\n",
    "gpt2.eval()\n",
    "inputs = tokenizer.encode(\"おはよう、お兄ちゃん。\", return_tensors=\"pt\").to(device)\n",
    "outputs = gpt2.generate(inputs, max_length=100, do_sample=True, top_p=0.95, top_k=60)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playgrounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([    9,  1010,  1406,  ..., 21433,   552,     2]), 'attention_mask': tensor([1, 1, 1,  ..., 1, 1, 1]), 'labels': tensor([    9,  1010,  1406,  ..., 21433,   552,     2])}\n"
     ]
    }
   ],
   "source": [
    "raw_dataset = Dataset('data/train.txt')\n",
    "for i in range(1):\n",
    "    print(raw_dataset[i])\n",
    "del raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "そろそろおやすみかな? 今日も一日、おつかれさま～。</s>...お兄ちゃん、いつもこんな時間まで起きてるの?</s>...まさか寝てないってことはないよね?</s> 朝ごはんはきちんと食べようね♪</s> 今日も一日、張り切っていこう!</s> 今起きたところっていう人もいるかな?</s> お昼休みはウキウキウォッチングだね♪</s> この時間帯は一番...眠くなるよね...。</s> 18時台のアニメのビデオ予約大丈夫!?忘れてない!?</s> 今日のお夕飯は何だろう? アレだったら嬉しいな～♪</s> 一般的なテレビも見ずに、積みゲーでも消化してるとこかな?</s> 今日はお兄ちゃんにとって、良い一日だった?</s> お兄ちゃん、おはよう!</s> こんにちは、お兄ちゃん。</s> こんばんは、お兄ちゃん。</s> ふーん...。水月先輩のえっちな画像、たくさん見たんだね...(汗</s> やった!じゃあ、最近覚えたての歌を歌うね。あー、コホン。パンパラパパンパンパンパラパパンパン(前奏)パンパラパパンパンパラッパパンパンパン(前奏)行こうよまぶしい光の世界♪ハートのスイッチオンにして～♪ごらんよ誰かが君を待ってる♪おんなじかたちの夢抱いて～♪あ～し～た～がす～き～な～ひ～と～だ～けが～ち～きゅ～う～をま～わ～す～♪ハッロー!ソフマッ ワー♪言葉はいらないほほえみあえば♪たちまち素敵なともだちさ～♪心と心を響かせあって♪愛を歌おうよウィーアーソフマッワー♪時間の流れをさあ追い抜いて♪迎えに行こうよしあわせを～♪誰にも見えない新しい道♪ひとあしお先に走るんだ～♪あ～つ～い～しせ～ん～の～ひ～と～だ～けが～ち～きゅ～う～をま～わ～す～♪ハッロー!ソフマッ ワー♪コバルト色したおおきな空は♪未来を映せるスクリーン♪心と心の絵の具を混ぜて♪愛を描こうよウィーアーソフマッワー♪おいでよ不思議が呼んでる世界♪ハートのスピードフルにして～♪君ならできるさ大人にな～ら♪時代にできない冒険が～♪じ～ゆ～う～のに～あ～う～ひ～と～だ～けが～ち～きゅ～う～をま～わ～す～♪ハッロー!ソフマッ ワー♪言葉はいらないほほえみあえば♪たちまち素敵なともだちさ～♪心と心を響かせあって♪愛を歌おうよウィーアーソフマッワー♪ウィーアーソフマッワー♪・・・・・・・・。...まさかフルコーラス歌えるとは思ってなかったでしょう?(ニヤソ</s> なによその、そっけない態度はぁ～...。ふーんだ!</s> そうだよね!こんぴゅーたさま!</s> え～、私○学生だしお金ないよぅ～...。<unk> (; ́д<unk> )ノ</s>..................。...よければ私が、あい...あ、あははっ、なんでもないよ!?</s>...なんかこの中地味だと思ったら、そういうことかぁ～。アプリ貯めこんでばっかいないで、電話やメールも使おうね(ニヤソ</s> ねぇ、お兄ちゃん。「ラブやん」って漫画がアニメになるって噂が立ってるけどやっぱガセかなぁ?お兄ちゃんはどう思う?</s> いつもクールなアカギ...いいよね♪あの生死を賭けたギャンブル...たまらないっ!!私、もしお金持ちになったら...お兄ちゃんに鷲巣麻雀セット買ってあげる!ざわ...ざわ...</s> やった!それでこそお兄ちゃんだねっ!</s> そういえばうちのお姉ちゃん、小林製薬に弱いんだよ...。テレビcmを見ては、「茜～、これ便利なんだよ～」って、『網戸きれいに仕上げ隊』とか『サカムケア』とか...、『なめらかかとスティック』や『爪ピカッシュ』なんかを買って来るんだ...。使う機会があれば、確かに便利なんだろうけど...。うち...、かかとのひび割れに困ってる人なんていないし、網戸だって今は充分綺麗なんだけどね...(しゅん)お兄ちゃんだって...もしお姉ちゃんを押し倒したときに、『あせワキパット』が出てきたら萎えるよね?(ニヤソ</s>\n",
      "1665\n",
      "================================================================================\n",
      "はあー...。はうぅぅ～...。ゲームキューブのlrトリガーをシャコシャコ押したい禁断症状だよ...。うずうず...。うずうず...。</s> うぅ～ん!今日も良い天気だねお兄ちゃん♪まるで、スーパーアゾのような青空だよ～!</s>...そっか.........。そ、そうだよね...。お兄ちゃんには...お姉ちゃんがいるもんね...(しゅん)..................。あ、えっとぉ...(汗</s> お兄ちゃん、お姉ちゃんのチョコ、ちゃんと食べてあげてね?</s> んもぉー!お兄ちゃんがくれるcd-rって、いつも海外製!たまには誘電とかで焼いてよ～!(<unk> □ ́)</s> お礼にたっぷりご奉仕するにゃん♪あいつら、意味わかってて言ってんのかしら?(<unk> д<unk> )y-~~</s> 連載ではもう、最終回を迎えてるんだよね...。実は私...まだ読んでないの...。単行本で読んでるから...。あ! お兄ちゃん、内容言っちゃダメ!</s> あっ、お兄ちゃん!実は今度うちの学校でね...、『学校へいこう!』の未成年の主張をやるんだ～♪私は何を主張しようかなぁ～?屋上から「ギャルのパンティお～くれ～!」とか言ったらウケるかな?...そもそもソ○倫対策的に私が未成年なのか謎だけどね(汗</s> わぁ～っ!お兄ちゃんって、ハイブリッド性ヲタなんだね～。</s> ねえ、お兄ちゃん!最近、レトロゲーがブームだけど、お兄ちゃんはどこの会社のレゲーが好きかな?</s> じゅ、重症だね(汗</s> えっ、知らないの...?(;;<unk> д<unk> )ガタガタ</s>...お兄ちゃん......。...ごめん。ちょっとキモイ!</s> (<unk> д<unk> )ウマー</s> 私うれしいけど...。...結構無言の圧力に弱いタイプなんだねお兄ちゃんは(汗</s> もぉ～、お兄ちゃんはノリが悪いなぁ...。</s> んーとね。金髪で目が釣りあがっていて...まさに鬼のような形相している娘だって!楽しみだねぇ♪</s> きゃー!情報発信源を検閲しなくちゃぁ!</s> わぁ～い、ありがと～う♪<unk> ( ́ー<unk> )ノ</s>...まさか近い将来、自分が手を染めることになろうとは...。あの時は思いもよらないお兄ちゃんなのであった...(完)</s>..................。...お姉ちゃんに相談しておこうか?(ニヤソ</s>...なんか処理速度遅いと思ったら、そういうことかぁ～。お兄ちゃんが気になるあのアプリ、ここじゃ動かないよ(ニヤソ</s> 7月20日、今日は海の日だね～、お兄ちゃん。題目はどうあれ、休めるのは良いことだよね♪</s> ふ～ん...。お兄ちゃん、意外と普通の人なんだね～...。</s> カイジいいよね♪私はやっぱり、絶望の橋が一番好きかな～。いつか、一緒にeカードやろうね!ざわ...ざわ...</s> ねえ、お兄ちゃん。「ちよれん」ではどこのメーカーさんが好き?</s> 現役高校生でこのiアプリをやってるようなお兄ちゃんは...青春をもっと正しい方向でエンジョイするべきだと思うな...(涙)</s> お兄ちゃんの持ってるゲーム機のコントローラー...女の子が触ったことないでしょう?(ニヤソ</s> じゃ～ん!今日はバレンタインだからお兄ちゃんには...義理チョコならぬ、義理茜ちゃんをプレゼントでーっす!...義理私って、一体なんだろ...(汗とにかく、義理っ!</s> 仲間に～♪入～れな～いわ～♪友達～よ～♪でも～♪親友じゃ～♪ないからぁ～♪</s> マジキュー・プレミアムを買って、ちゃんと全ページ読んでる人って私、ある意味尊敬しちゃうかも...。問い詰めたい。小一時間問い詰めたい。あなたはそんなに萌えが不足しているのかと。</s> こんぴゅーたさま!携帯javaに誤り(バグ)があるってほんとうなのかな?</s> はじめまして、私の名前は涼宮茜!「茜たん様」って呼んでもいいよ?...でも、「たん様」って略されたらちょっとやだな(汗なにはともあれ、これからよろしく、お兄ちゃん。パケット代に注意しつつ、いっぱい私に会いに来てね♪ちなみに、このiアプリ中に登場するキャラクターや\n",
      "1794\n",
      "================================================================================\n",
      "話題は全て架空のもので、実在のものとは一切関係ないから...。...一応、お約束ってことで胸の内にしまっておいてね♪</s>...そんなこと言っても、お兄ちゃんバレバレだよ.........。</s> zapzapzap!!!キミはレーザピストルで撃たれた。</s> エロゲーのエッチシーンの訪れのほうが、よっぽど唐突だよ!</s>...今日の私も、明日の私も。半年後の私も、一年後の私も。いつもこんなステキな毎日を送れますように。そんな想いを明日に託し、さよならをする今日の私。そしてそんな想いを抱いて生まれる明日の私。私はきっと、明日もお兄ちゃんの元へと会いに行く。明日お兄ちゃんに会ったら、最初に何て言ってみよう?明日の私は生まれたての新しい私。だから久しぶりに...こんな挨拶もいいかもしれない。はじめまして、お兄ちゃん!</s> えへへ...。正直なお兄ちゃん、私嫌いじゃないよ♪打倒! しおさお!</s> うわ!お兄ちゃんシブイ!やっぱり銃とスリルとバイオレンスが燃える!?あそこの広報さんは近くで見るとちょっとコワイかも(汗)ちなみに、私とモーラちゃんはお友達同士なんだよ♪</s> かー!さすがお兄ちゃん!萌えよりも燃えを重視するんだねぇ...。でも、ちょっと意外...(ニヤソ</s>...そうなんだ......。水月先輩、すっごくいい人だから...。私も応援するね♪</s> タンッ!ジャキン!ズバッ!ジャコン!...ふっ.........。またドコモを儲けさせてしまった...(ニヤソ</s> 私もよく部活の後なんかに友達と吉牛行くんだけど、つゆだくもねぎだくも、もう時代遅れなんだよね。今最も熱いオプションは『肉だく』。これだね♪肉だくはその名の通り、肉をたくさん入れてもらう。これ。女子水泳部一同、甘いボイスで並・肉だくを注文。しかし店員が女性だときっちり大盛り料金取られたりもする諸刃の剣。素人にはお勧めできない。まあお兄ちゃんは家でカップ麺でも食べてなさいってことね。</s> あぁぁぁ...!寝ている間に...ネズミにアホ毛をかじられたぁ～!...鬱だ、snow......。</s> ねえ、お兄ちゃん♪次の中で、誰の歌が一番好きなのかな?</s> 実は昨日ね...。日本妹キャラクター連盟の集会に行ってきたんだけど...。高屋敷さんにちょっと酷いこと言っちゃった...。...今度会ったらちゃんと謝っとこ......。(しゅん)</s> もし街でマリ姉を見かけて、「合言葉は?」って聞いたら...「ビィ～!!」ってミラクルな笑顔で答えてくれるかなぁ?</s> んもー!雪印のばかぁっ!!なんだかんだと、とやかく言っても...雪印のコーヒー牛乳が一番美味しいんだよっ!それなのに...それなのに...またお店から無くなって飲めない日々だよ...!</s> う～ん...、じゃあお姉ちゃんかな...?</s> えっとぉ...、それは誰なのかな?お兄ちゃんの裏切り者ぉ!胸が～震える～♪(ダッシュ逃げ)</s> え!?...お兄ちゃん......。いきなりそんなこと言われても...、私、困るよ.........。でも...私も...、お兄ちゃんのこと...好き...だよ.........。........................。あっ、当然、『like』だからね♪</s> お兄ちゃん、割とマニアックなんだね...。やっぱり、おなかが大きいと萌える?ちなみに、私とマヨちゃんはお友達同士なんだよ♪</s> あの羽で飛んでたいやき屋をふり切るのかな?やっぱり食い逃げのプロは違うね～。</s> う～ん、そっか...。お兄ちゃんいつも夜遅いみたいだし、ちょっと辛いよね...。</s> 今日は、うちのお姉ちゃんの誕生日～!わ～、パチパチパチ...。...お兄ちゃん、まさか忘れてたなんてこと...ないよね?そんなわけで今日は特別にお姉ちゃんの恥ずかしい秘密を...茜っ!!わあっ!?お、お姉ちゃん!?ごっ...、ごめんなさぁ～い...(ダッシュ)...もぉ～、茜ったら何言おうとしてたのよぉ～(汗うぅ～、いつも茜の相手してくれて、ごめんね...。あんな妹だけど...、これからもよろしくしてあげてね?</s>...う～\n",
      "1777\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "datamodule = DataModule(\n",
    "    train='data/train.txt',\n",
    "    val='data/train.txt',\n",
    "    test='data/train.txt',\n",
    "    batch_size=1,\n",
    ")\n",
    "\n",
    "datamodule.setup()\n",
    "for batch in datamodule.train_dataloader():\n",
    "    x, y = batch\n",
    "    texts = tokenizer.batch_decode(x)\n",
    "    for text in texts:\n",
    "        print(text)\n",
    "        print(len(text))\n",
    "        print(\"=\"*80)\n",
    "\n",
    "del datamodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.7355, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([3, 1024, 32000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-24 12:36:37.413314: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-24 12:36:37.482720: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-08-24 12:36:37.775123: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-08-24 12:36:37.775151: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-08-24 12:36:37.775154: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input が出てきたら萎えるよね?(ニヤソ</s>\n",
      "output ろそろ、正月つなさい? とは一日おお疲れかれさまで。 今日今日 やちゃん、おありがとうにまで起きてるの?......おか、坊ないよことはないよね?......ごはんは、食べてるね。......も一日、お切ってがね...夜らは、のけど?...やごは、チウキ気分ング♪ね♪...時間は、お い時間帯ね?。...時はウは再放送をは丈夫かな ずにた?......お昼飯はに?...?ら、しいな♪。......おの、れる、このゲーをしてしよう、かかな?......もお休みちゃんの、特別な日になりかな...やちゃん、今日! 今日今日は!お兄ちゃん。 今日今日ばんは、お兄ちゃん。 今日今日ふん、。 着。、ことはっちな妄想が見見ことね。。笑)今日今日っぱー あ、今日はたのをよ♪ 、、今日ツ。 ツのッパパパンパンパンパンパンパン笑略曲 パラパパンパンパンパンパンパンパンパン後奏)パンけよ!ーしいよがへ の光を!、♪ めん、、に見のるよ なじ空っこ星をいてる♪ 、、、、ぁいんばごくく～♪～いなん～♪～い～～?～ん～～ん～♪～～ん～ん♪♪ ハッ! レバンチョ...イ がいらないよ～えみ♪～いい 、敵な世界だちになれ～ん が心つかせて～ をこおう♪♪アー!フマッ♪♪ よをゆような、かけて! に行こうよよしよう歌♪ にも邪魔から世界へ ときし先に♪よよ♪ ～、～い～い～～♪♪♪～♪～と～ん～けが～ち～きゅ～い～♪ま～わ～す～♪ ロー!ソフマッ ワー♪言葉ブルーに空きな瞳を、 の照らしす鏡の と心をがを具♪のて、心を歌こうよウィーアーソフマッワー♪心でよ誰かのいっぱいる♪♪ハートのスイッチをな～♪あのきよ♪のなればっちゃれ♪ 流ことをを待♪ ぶんん～わ～ん～～♪～ん～の～と～を～けが～ち～きゅ～う～をま～わ～す～♪ ロー!ソフマッ ワー♪ はいらないほほえみあれば♪たちまち素敵なともだちさ～♪心と心を描かせあって♪愛を描おうよウィーアーソフマッワー♪おいアーソフマッワー♪ほ・・・・・・・・ あか、ネームでえるとは思わなかったなかったよね 笑リ)))～を、おおろとして顔は?～?。 ふん、 ああいね、 ばんら～ん～～ そうそうっちっ今日もちゃんの、もんだね～。。 (<unk> ́д<unk> )ノ でもでも........................ぉ、おおつ 、あ、はは!はかわぉ あ......あか、前に入ってがなねら、こんなことかぁ～。 のめてとでるっかりで、もっとしてメールでちゃんってよ♪笑ソ なあぇ、お兄ちゃん、 おライブん」って言って、気でって知は...るんだ、っぱりセだよ?? 兄ちゃん、、思う? ああのようになのネちゃん。なねぇ 、意気を賭けた戦いは。ぶんよ! もあのもがだったら、。兄ちゃんに会い掴くんををってあげるから...あ...ざわ.........った!じゃじゃ、、兄ちゃん!ね♪♪ ああだよえば、の兄ちゃん、おさんの勤めってよね。 でで、、あちゃん」茜、だだよね」って言って茜膜』』するて』って『網プリイ』とか言って。おらか』にろ』とか『お切りピカ』とかかをくるんだよ。 のがない、教えに便利だだよけど、。 のお おあぁん...割れが悩ってるんだにはていないよ、そんながのって、は便利に麗にのけど、...。汗) 兄ちゃん、ね、。しか兄ちゃんがのら、あ、～ザ』』とか入ってら、えるかもしれない...... 笑ソ な\n"
     ]
    }
   ],
   "source": [
    "datamodule = DataModule(\n",
    "    train='data/train.txt',\n",
    "    val='data/train.txt',\n",
    "    test='data/train.txt',\n",
    ")\n",
    "datamodule.setup()\n",
    "\n",
    "try:\n",
    "    model = GPT2().to('cuda')\n",
    "    for batch in datamodule.train_dataloader():\n",
    "        batch = [x.to('cuda') for x in batch]\n",
    "        x, y = batch\n",
    "        \n",
    "        outputs = model(x, y)\n",
    "        print(outputs.loss)\n",
    "        print(outputs.logits.shape)\n",
    "        print(\"input\", tokenizer.decode(x[0])[-20:])\n",
    "        print(\"output\", tokenizer.decode(outputs.logits[0].argmax(dim=-1).tolist()))\n",
    "        break\n",
    "except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'こんにちは</s>'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.encode(\"こんにちは\", return_tensors=\"pt\").to(device)\n",
    "tokenizer.decode(tokens[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt_rinna_finetuning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

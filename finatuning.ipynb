{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "from transformers import T5Tokenizer, AutoModelForCausalLM\n",
                "import torch"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "'NVIDIA GeForce RTX 4090'"
                        ]
                    },
                    "execution_count": 9,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "torch.cuda.get_device_name(torch.device('cuda:0'))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Cloning into 'transformers'...\n",
                        "remote: Enumerating objects: 157524, done.\u001b[K\n",
                        "remote: Counting objects: 100% (2545/2545), done.\u001b[K\n",
                        "remote: Compressing objects: 100% (1014/1014), done.\u001b[K\n",
                        "remote: Total 157524 (delta 1581), reused 2037 (delta 1286), pack-reused 154979\u001b[K\n",
                        "Receiving objects: 100% (157524/157524), 159.55 MiB | 24.78 MiB/s, done.\n",
                        "Resolving deltas: 100% (117762/117762), done.\n",
                        "Note: switching to 'bd469c40659ce76c81f69c7726759d249b4aef49'.\n",
                        "\n",
                        "You are in 'detached HEAD' state. You can look around, make experimental\n",
                        "changes and commit them, and you can discard any commits you make in this\n",
                        "state without impacting any branches by switching back to a branch.\n",
                        "\n",
                        "If you want to create a new branch to retain commits you create, you may\n",
                        "do so (now or later) by using -c with the switch command. Example:\n",
                        "\n",
                        "  git switch -c <new-branch-name>\n",
                        "\n",
                        "Or undo this operation with:\n",
                        "\n",
                        "  git switch -\n",
                        "\n",
                        "Turn off this advice by setting config variable advice.detachedHead to false\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "!git clone https://github.com/huggingface/transformers -b v4.23.1"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "08/23/2023 17:13:45 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
                        "08/23/2023 17:13:45 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
                        "_n_gpu=1,\n",
                        "adafactor=False,\n",
                        "adam_beta1=0.9,\n",
                        "adam_beta2=0.999,\n",
                        "adam_epsilon=1e-08,\n",
                        "auto_find_batch_size=False,\n",
                        "bf16=False,\n",
                        "bf16_full_eval=False,\n",
                        "data_seed=None,\n",
                        "dataloader_drop_last=False,\n",
                        "dataloader_num_workers=0,\n",
                        "dataloader_pin_memory=True,\n",
                        "ddp_bucket_cap_mb=None,\n",
                        "ddp_find_unused_parameters=None,\n",
                        "ddp_timeout=1800,\n",
                        "debug=[],\n",
                        "deepspeed=None,\n",
                        "disable_tqdm=False,\n",
                        "do_eval=True,\n",
                        "do_predict=False,\n",
                        "do_train=True,\n",
                        "eval_accumulation_steps=None,\n",
                        "eval_delay=0,\n",
                        "eval_steps=None,\n",
                        "evaluation_strategy=no,\n",
                        "fp16=False,\n",
                        "fp16_backend=auto,\n",
                        "fp16_full_eval=False,\n",
                        "fp16_opt_level=O1,\n",
                        "fsdp=[],\n",
                        "fsdp_min_num_params=0,\n",
                        "fsdp_transformer_layer_cls_to_wrap=None,\n",
                        "full_determinism=False,\n",
                        "gradient_accumulation_steps=1,\n",
                        "gradient_checkpointing=False,\n",
                        "greater_is_better=None,\n",
                        "group_by_length=False,\n",
                        "half_precision_backend=auto,\n",
                        "hub_model_id=None,\n",
                        "hub_private_repo=False,\n",
                        "hub_strategy=every_save,\n",
                        "hub_token=<HUB_TOKEN>,\n",
                        "ignore_data_skip=False,\n",
                        "include_inputs_for_metrics=False,\n",
                        "jit_mode_eval=False,\n",
                        "label_names=None,\n",
                        "label_smoothing_factor=0.0,\n",
                        "learning_rate=5e-05,\n",
                        "length_column_name=length,\n",
                        "load_best_model_at_end=False,\n",
                        "local_rank=-1,\n",
                        "log_level=passive,\n",
                        "log_level_replica=passive,\n",
                        "log_on_each_node=True,\n",
                        "logging_dir=output/runs/Aug23_17-13-45_ouranos-Alienware-Aurora-R15,\n",
                        "logging_first_step=False,\n",
                        "logging_nan_inf_filter=True,\n",
                        "logging_steps=500,\n",
                        "logging_strategy=steps,\n",
                        "lr_scheduler_type=linear,\n",
                        "max_grad_norm=1.0,\n",
                        "max_steps=-1,\n",
                        "metric_for_best_model=None,\n",
                        "mp_parameters=,\n",
                        "no_cuda=False,\n",
                        "num_train_epochs=50.0,\n",
                        "optim=adamw_hf,\n",
                        "output_dir=output/,\n",
                        "overwrite_output_dir=False,\n",
                        "past_index=-1,\n",
                        "per_device_eval_batch_size=1,\n",
                        "per_device_train_batch_size=1,\n",
                        "prediction_loss_only=False,\n",
                        "push_to_hub=False,\n",
                        "push_to_hub_model_id=None,\n",
                        "push_to_hub_organization=None,\n",
                        "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
                        "ray_scope=last,\n",
                        "remove_unused_columns=True,\n",
                        "report_to=[],\n",
                        "resume_from_checkpoint=None,\n",
                        "run_name=output/,\n",
                        "save_on_each_node=False,\n",
                        "save_steps=5000,\n",
                        "save_strategy=steps,\n",
                        "save_total_limit=3,\n",
                        "seed=42,\n",
                        "sharded_ddp=[],\n",
                        "skip_memory_metrics=True,\n",
                        "tf32=None,\n",
                        "torchdynamo=None,\n",
                        "tpu_metrics_debug=False,\n",
                        "tpu_num_cores=None,\n",
                        "use_ipex=False,\n",
                        "use_legacy_prediction_loop=False,\n",
                        "use_mps_device=False,\n",
                        "warmup_ratio=0.0,\n",
                        "warmup_steps=0,\n",
                        "weight_decay=0.0,\n",
                        "xpu_backend=None,\n",
                        ")\n",
                        "08/23/2023 17:13:46 - INFO - datasets.builder - Using custom data configuration default-783c9ea4e23b1e7d\n",
                        "08/23/2023 17:13:46 - INFO - datasets.info - Loading Dataset Infos from /home/zacker/anaconda3/envs/gpt_rinna_finetuning/lib/python3.7/site-packages/datasets/packaged_modules/text\n",
                        "08/23/2023 17:13:46 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
                        "08/23/2023 17:13:46 - INFO - datasets.info - Loading Dataset info from /home/zacker/.cache/huggingface/datasets/text/default-783c9ea4e23b1e7d/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2\n",
                        "08/23/2023 17:13:46 - WARNING - datasets.builder - Found cached dataset text (/home/zacker/.cache/huggingface/datasets/text/default-783c9ea4e23b1e7d/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n",
                        "08/23/2023 17:13:46 - INFO - datasets.info - Loading Dataset info from /home/zacker/.cache/huggingface/datasets/text/default-783c9ea4e23b1e7d/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2\n",
                        "100%|███████████████████████████████████████████| 2/2 [00:00<00:00, 2222.15it/s]\n",
                        "[INFO|configuration_utils.py:653] 2023-08-23 17:13:46,907 >> loading configuration file config.json from cache at /home/zacker/.cache/huggingface/hub/models--rinna--japanese-gpt2-medium/snapshots/3170081b160e59488754c827128b9c67efcb90be/config.json\n",
                        "[INFO|configuration_utils.py:705] 2023-08-23 17:13:46,908 >> Model config GPT2Config {\n",
                        "  \"_name_or_path\": \"rinna/japanese-gpt2-medium\",\n",
                        "  \"activation_function\": \"gelu_new\",\n",
                        "  \"architectures\": [\n",
                        "    \"GPT2LMHeadModel\"\n",
                        "  ],\n",
                        "  \"attn_pdrop\": 0.1,\n",
                        "  \"bos_token_id\": 1,\n",
                        "  \"embd_pdrop\": 0.1,\n",
                        "  \"eos_token_id\": 2,\n",
                        "  \"gradient_checkpointing\": false,\n",
                        "  \"initializer_range\": 0.02,\n",
                        "  \"layer_norm_epsilon\": 1e-05,\n",
                        "  \"model_type\": \"gpt2\",\n",
                        "  \"n_ctx\": 1024,\n",
                        "  \"n_embd\": 1024,\n",
                        "  \"n_head\": 16,\n",
                        "  \"n_inner\": 4096,\n",
                        "  \"n_layer\": 24,\n",
                        "  \"n_positions\": 1024,\n",
                        "  \"reorder_and_upcast_attn\": false,\n",
                        "  \"resid_pdrop\": 0.1,\n",
                        "  \"scale_attn_by_inverse_layer_idx\": false,\n",
                        "  \"scale_attn_weights\": true,\n",
                        "  \"summary_activation\": null,\n",
                        "  \"summary_first_dropout\": 0.1,\n",
                        "  \"summary_proj_to_labels\": true,\n",
                        "  \"summary_type\": \"cls_index\",\n",
                        "  \"summary_use_proj\": true,\n",
                        "  \"task_specific_params\": {\n",
                        "    \"text-generation\": {\n",
                        "      \"do_sample\": true,\n",
                        "      \"max_length\": 50\n",
                        "    }\n",
                        "  },\n",
                        "  \"transformers_version\": \"4.23.1\",\n",
                        "  \"use_cache\": true,\n",
                        "  \"vocab_size\": 32000\n",
                        "}\n",
                        "\n",
                        "[INFO|tokenization_utils_base.py:1773] 2023-08-23 17:13:47,082 >> loading file spiece.model from cache at /home/zacker/.cache/huggingface/hub/models--rinna--japanese-gpt2-medium/snapshots/3170081b160e59488754c827128b9c67efcb90be/spiece.model\n",
                        "[INFO|tokenization_utils_base.py:1773] 2023-08-23 17:13:47,082 >> loading file tokenizer.json from cache at None\n",
                        "[INFO|tokenization_utils_base.py:1773] 2023-08-23 17:13:47,082 >> loading file added_tokens.json from cache at None\n",
                        "[INFO|tokenization_utils_base.py:1773] 2023-08-23 17:13:47,082 >> loading file special_tokens_map.json from cache at /home/zacker/.cache/huggingface/hub/models--rinna--japanese-gpt2-medium/snapshots/3170081b160e59488754c827128b9c67efcb90be/special_tokens_map.json\n",
                        "[INFO|tokenization_utils_base.py:1773] 2023-08-23 17:13:47,082 >> loading file tokenizer_config.json from cache at /home/zacker/.cache/huggingface/hub/models--rinna--japanese-gpt2-medium/snapshots/3170081b160e59488754c827128b9c67efcb90be/tokenizer_config.json\n",
                        "[INFO|modeling_utils.py:2156] 2023-08-23 17:13:47,146 >> loading weights file model.safetensors from cache at /home/zacker/.cache/huggingface/hub/models--rinna--japanese-gpt2-medium/snapshots/3170081b160e59488754c827128b9c67efcb90be/model.safetensors\n",
                        "[INFO|modeling_utils.py:2606] 2023-08-23 17:13:48,362 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
                        "\n",
                        "[INFO|modeling_utils.py:2615] 2023-08-23 17:13:48,362 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at rinna/japanese-gpt2-medium.\n",
                        "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
                        "08/23/2023 17:13:48 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/zacker/.cache/huggingface/datasets/text/default-783c9ea4e23b1e7d/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-225bc3d2c31e1cb6.arrow\n",
                        "08/23/2023 17:13:48 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/zacker/.cache/huggingface/datasets/text/default-783c9ea4e23b1e7d/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-bcb2d6299b2c7022.arrow\n",
                        "08/23/2023 17:13:48 - WARNING - __main__ - The tokenizer picked seems to have a very large `model_max_length` (1000000000000000019884624838656). Picking 1024 instead. You can change that default value by passing --block_size xxx.\n",
                        "08/23/2023 17:13:48 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/zacker/.cache/huggingface/datasets/text/default-783c9ea4e23b1e7d/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-dc84b71f9ea4ce75.arrow\n",
                        "08/23/2023 17:13:48 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/zacker/.cache/huggingface/datasets/text/default-783c9ea4e23b1e7d/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-e6fb3d9c269ba2a2.arrow\n",
                        "/home/zacker/anaconda3/envs/gpt_rinna_finetuning/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
                        "  FutureWarning,\n",
                        "[INFO|trainer.py:1607] 2023-08-23 17:13:50,279 >> ***** Running training *****\n",
                        "[INFO|trainer.py:1608] 2023-08-23 17:13:50,279 >>   Num examples = 14\n",
                        "[INFO|trainer.py:1609] 2023-08-23 17:13:50,279 >>   Num Epochs = 50\n",
                        "[INFO|trainer.py:1610] 2023-08-23 17:13:50,279 >>   Instantaneous batch size per device = 1\n",
                        "[INFO|trainer.py:1611] 2023-08-23 17:13:50,279 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
                        "[INFO|trainer.py:1612] 2023-08-23 17:13:50,279 >>   Gradient Accumulation steps = 1\n",
                        "[INFO|trainer.py:1613] 2023-08-23 17:13:50,279 >>   Total optimization steps = 700\n",
                        "{'loss': 0.6414, 'learning_rate': 1.4285714285714285e-05, 'epoch': 35.71}       \n",
                        "100%|█████████████████████████████████████████| 700/700 [01:39<00:00,  7.05it/s][INFO|trainer.py:1852] 2023-08-23 17:15:29,562 >> \n",
                        "\n",
                        "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
                        "\n",
                        "\n",
                        "{'train_runtime': 99.2847, 'train_samples_per_second': 7.05, 'train_steps_per_second': 7.05, 'train_loss': 0.4681687579836164, 'epoch': 50.0}\n",
                        "100%|█████████████████████████████████████████| 700/700 [01:39<00:00,  7.05it/s]\n",
                        "[INFO|trainer.py:2656] 2023-08-23 17:15:29,564 >> Saving model checkpoint to output/\n",
                        "[INFO|configuration_utils.py:447] 2023-08-23 17:15:29,564 >> Configuration saved in output/config.json\n",
                        "[INFO|modeling_utils.py:1624] 2023-08-23 17:15:30,340 >> Model weights saved in output/pytorch_model.bin\n",
                        "[INFO|tokenization_utils_base.py:2123] 2023-08-23 17:15:30,341 >> tokenizer config file saved in output/tokenizer_config.json\n",
                        "[INFO|tokenization_utils_base.py:2130] 2023-08-23 17:15:30,341 >> Special tokens file saved in output/special_tokens_map.json\n",
                        "[INFO|tokenization_t5_fast.py:187] 2023-08-23 17:15:30,354 >> Copy vocab file to output/spiece.model\n",
                        "***** train metrics *****\n",
                        "  epoch                    =       50.0\n",
                        "  train_loss               =     0.4682\n",
                        "  train_runtime            = 0:01:39.28\n",
                        "  train_samples            =         14\n",
                        "  train_samples_per_second =       7.05\n",
                        "  train_steps_per_second   =       7.05\n",
                        "08/23/2023 17:15:30 - INFO - __main__ - *** Evaluate ***\n",
                        "[INFO|trainer.py:2907] 2023-08-23 17:15:30,358 >> ***** Running Evaluation *****\n",
                        "[INFO|trainer.py:2909] 2023-08-23 17:15:30,358 >>   Num examples = 14\n",
                        "[INFO|trainer.py:2912] 2023-08-23 17:15:30,358 >>   Batch size = 1\n",
                        "100%|███████████████████████████████████████████| 14/14 [00:00<00:00, 27.87it/s]\n",
                        "***** eval metrics *****\n",
                        "  epoch                   =       50.0\n",
                        "  eval_accuracy           =     0.9986\n",
                        "  eval_loss               =     0.0121\n",
                        "  eval_runtime            = 0:00:00.54\n",
                        "  eval_samples            =         14\n",
                        "  eval_samples_per_second =     25.897\n",
                        "  eval_steps_per_second   =     25.897\n",
                        "  perplexity              =     1.0122\n",
                        "[INFO|modelcard.py:444] 2023-08-23 17:15:31,138 >> Dropping the following result as it does not have all the necessary fields:\n",
                        "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.9986035469906438}]}\n",
                        "CPU times: user 721 ms, sys: 145 ms, total: 866 ms\n",
                        "Wall time: 1min 47s\n"
                    ]
                }
            ],
            "source": [
                "%%time\n",
                "\n",
                "!rm -r output\n",
                "\n",
                "!python ./transformers/examples/pytorch/language-modeling/run_clm.py \\\n",
                "    --model_name_or_path=rinna/japanese-gpt2-medium \\\n",
                "    --train_file=data/train.txt \\\n",
                "    --validation_file=data/train.txt \\\n",
                "    --do_train \\\n",
                "    --do_eval \\\n",
                "    --num_train_epochs=50 \\\n",
                "    --save_steps=5000 \\\n",
                "    --save_total_limit=3 \\\n",
                "    --per_device_train_batch_size=1 \\\n",
                "    --per_device_eval_batch_size=1 \\\n",
                "    --output_dir=output/"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [],
            "source": [
                "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "tokenizer = T5Tokenizer.from_pretrained(\"rinna/japanese-gpt2-medium\")\n",
                "model = AutoModelForCausalLM.from_pretrained(\"output/\").to(device)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
                        "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
                        "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "['おはよう</s> おかえり こんばんは こんにちは こんばんは おやすみ こんにちは こんにちは こんばんは こんばんは こんばんは こんばんは こんぴゅーたさま! えへへ...  ちぇっ! もぉ～! もぉ～! もぉ～! もぉ～!...終電逃しちゃったよぉ～...(;;<unk> )ガタガ',\n",
                            " 'おはよう</s> おやすみ こんにちは こんばんわ こんにちわ! こんばんわ! こんばんわっ! ちぇっ! ちぇっ! ちぇっ!  やっ...、こんばんは! えへへ...、こ、これ...、くれる...?...............。...ごめん。私、食べちゃった(汗 </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>',\n",
                            " 'おはよう</s> ございまひる! 今日も清々しい一日のはじまりだね、お兄ちゃん!  にこにこ、はりきってあいさつしようね♪ ...つくしごはん、たのしいね♪ ...つくしごはんって、食べておいしいものじゃないの...? </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>',\n",
                            " 'おはよう</s> こんにちは こんばんは ええじいさん! ヨロシク!   ふ～ん、平さん?  ふ～ん、なんか嫌なことあったの? はうぅ～...(しゅん)  なによその、そっけない態度はぁ～...(汗  </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>',\n",
                            " 'おはよう</s> わっしょい がんばれ! みかんたえ者 チャッピー チャッピー朝太郎 チャッピーひな祭り チャッピーおにいちゃん チャッピー月見草 チャッピーのわんぱく大冒険 チャッピーの大冒険2 チャッピーのわんぱく大冒険3 チャッピーのわんぱく大冒険4 チャッピーのだいじょうぶだね チャッピーのだいじょうぶだいいな</s> </s> </s>',\n",
                            " 'おはよう</s> わーい、はやあそこのあゆちゃん! あかねと～くはね～♪ ねぇねぇ、お兄ちゃん。『茜たん』って漫画がすごく好きだよね♪♪ わ～い、やっぱりお兄ちゃん大好き! あかねちゃんはお姉ちゃんにしてあげる!  う～ん、今度会ったら結界創ってもらおうかな...。 </s> </s> </s> </s>',\n",
                            " 'おはよう</s> ございまひる! ( )ちいさいおうち したまちコメディ映画祭in台東 開会式 選手入場 朝ごはんばんざい!  ( )ちいさいおうち したまちコメディ映画祭 開会式 選手入場 おはようございまひる! ( )まちいきいきいき こんにちは こんばんは こんばんは こんばんは こんばんは こんばんは こんばん',\n",
                            " 'おはよう</s> こんばんは ちぇっ! やったぁ! やったぁ! やったぁ! やったぁ! やったぁ! やったぁ! やったぁ! やったぁ～! やったぁ! やったぁ! やったぁ～! やったぁ～～! やったぁ～～!! やったぁ～～～! やったぁ～～～! や']"
                        ]
                    },
                    "execution_count": 12,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "input = tokenizer.encode(\"おはよう\", return_tensors=\"pt\").to(device)\n",
                "output = model.generate(input, do_sample=True, max_length=100, num_return_sequences=8)\n",
                "tokenizer.batch_decode(output)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "gpt_rinna_finetuning",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.7.13"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
